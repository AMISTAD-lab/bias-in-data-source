<!DOCTYPE html>
<html>
<head>
  <title>Identifying Bias in Data using Two-Distribution Hypothesis Tests - AMISTAD Lab</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <link rel="stylesheet" type="text/css" href="projects.css">
  <link rel="stylesheet" type="text/css" href="project-example-specific.css">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <!-- Google Analytics setup -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-83215098-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- MathJax Setup -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
<div div style="margin: auto;" class="main-page-box">
  <!-- AMISTAD x NSF x HMC -->
  <a href="../amistad.html"><img src="./images/amistad-logo.png" alt="AMISTAD Logo" class="header-logo" /></a>

  <h1>Identifying Bias in Data using Two-Distribution Hypothesis Tests</h1>
  <h2 class="subtitle">2021</h2>
  <h3>Overview</h3>
  <p>
	Imagine you’re flipping a coin you presume to be fair. You flip it 20 times and see the sequence
	$$H T T H T T H H H H H T H T H T H H T H.$$
	Nothing immediately stands out to you, so you never question whether the coin is fair or not. But imagine you flipped that same coin and saw the sequence
	$$H H H H H H H H H H H H H H H H H H H H.$$
	You’d immediately suspect something was up. More specifically, you’d almost assuredly say that the coin was unfairly weighted towards heads. But how did you come to this conclusion? Was it based on probability alone? If the coin was truly fair, both of the heads-tails sequences above have exactly a 1/(2^20) chance of happening, so what makes you suspicious of the second sequence?
	<br><br>
	Humans are pretty good at recognizing patterns, and that very pattern recognition probably gave you a hint that a weighted coin generated the second sequence. It wasn’t just the low probability of that specific sequence occurring that raised your eyebrows. It was also that sequence’s conformity to some pattern. Our team is interested in making use of these two factors, probability and conformity, to analyze machine learning training data, and identify the potentially biased processes that could have generated them.
  </p>

  <!-- Background -->
  <h3>Background</h3>
  <p>
	Statistical hypothesis tests allow people to probabilistically rule out a proposed hypothesis as an explanation for the data. For example, a binomial test could be used to test the hypothesis that our coin in the previous example was fair. Such hypothesis tests work by generating a \(p\)-value, the probability that the observation (or a more extreme result) would occur under the proposed hypothesis. Typically, if this \(p\)-value is less than some chosen significance threshold alpha, then we reject the null hypothesis. Otherwise, we fail to reject it.
	<br><br>
	Several popular hypothesis tests, such as binomial, chi-square, and Student’s t, are widely used for testing the plausibility of null hypotheses. Some of these could be applicable for identifying bias in machine learning training data. Say a company wanted to train an algorithm to automate their hiring process. They would want to be able to feed this algorithm an applicant’s demographic information, background, and qualifications and have it return a result of “hire” or “do not hire”. In order to train this model, they decide to use their current and past hiring records, where each current or past applicant has an information profile, and a binary label of “hired” or “not hired”. Unfortunately, the company suspects that their historical hiring records might be unjustly biased towards white applicants. Here we take bias to mean deviation from some proposed unbiased explanation. In this case, the unbiased explanation is that racial demographics of the hired employees of the company match that of the entire pool of applicants, so no favor is shown to one race over another. A statistical hypothesis test (specifically chi-square in this case) could be used to determine whether this unbiased explanation would plausibly explain the demographics of the hired applicants. By plausible, we mean that the hypothesis was not rejected by the test, not necessarily that the hypothesis has a “high” chance of producing the data.
	<br><br>
	While all the hypothesis tests mentioned above are useful, all of them assume that sequences are generated under a particular known distribution. This distribution usually is just the name of the test. For a chi square test, we assume a chi square distribution. Additionally, these tests can tell you whether or not the proposed hypothesis could plausibly explain the data, but do not provide further insight. In many cases, if someone’s proposed hypothesis was rejected, they would probably be interested in answering the question of what hypothesis would not be rejected. For the company mentioned above, if their proposed unbiased explanation was rejected, they may wonder what explanation(s) would not be rejected, and how “close” their proposed hypothesis was to them.
	<br><br>
	That’s where our research comes in. Previous work from our lab has generated a novel hypothesis test that resolves the issues discussed above. We extend this work and show useful, real-world applications of the tests for industrial use.
  </p>
  
  <!-- Definitions -->
  <h3>Key Definitions and Methodology</h3>
  <p>
	Define the kardis \(\kappa(x)\) of event \(x\) as
	$$\kappa(x) := r\frac{p(x)}{\nu(x)}.$$
	where \(p(x)\) is the probability of observing \(x\) under probability distribution \(P\) (which represents the null hypothesis), \(\nu(x)\) is the <i>specificity</i> (conformity to a pattern) of \(x\) by some predetermined notion of structure, and \(r\) is a normalizing constant. Notice that this formulation of \(\kappa(x)\) captures both the probability and conformity of an event as discussed in the previous coin example. 
	<br><br>
	With this in mind, our lab has shown that for some chosen significance level \(\alpha\)
	$$\Pr(\kappa(x) \leq \alpha) \leq \alpha.$$
	Thus, we can reject a proposed hypothesis \(P\) if \(\kappa(x) \leq \alpha\). Furthermore, it has also been shown that in order for any new hypothesis to not be rejected by our test, it must boost \(p(x)\), the probability of observing \(x\), by <i>at least</i> a factor of
	$$s = \frac{\alpha\nu(x)}{rp(x)}.$$
	This bound is the key to our results, since it means that a proposed probablity distribution (explanation) \(Q\) must give at least
	$$q(x) \geq sp(x)$$
	probability to \(x\) in order to not be rejected by our hypothesis test.
	<br><br>
	Most importantly, this bound allows us to find the "closest feasible explanation" for the data in the case that the original proposed hypothesis is rejected by solving a constrained optimization problem. In other words, if a proposed hypothesis is rejected, we can say that the true explanation would have to be <i>biased</i> by at least a certain amount. This gives our test a unique degree of clarity, since it allows users to get a better sense of what hidden biases might be present in the processes that created their data.
	<br><br>
	It should be stated clearly that we are not making claims about bias of any particular algorithm, or even identifying algorithmic bias at all. Rather, we seek to uncover biases in the <i>processes</i> that generate training data, like hiring practices or weighted coins.
  </p>

  <!-- Results -->
  <h3>Selected Results</h3>
  <p>
	We tested our methods on three real-world datasets, two of which will be shown here. The first dataset we tested for bias in was the publicly available <a href="https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis" target="_blank">COMPAS</a> (Correctional Offender Management Profiling for Alternative Sanctions) dataset which was used to train the COMPAS algorithm which gives defendants in court a risk score for recidivism (tendency to reoffend). 
	<br><br>
	It is already <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" target="_blank">well known</a> that the algorithm is biased against Blacks, consistently giving them unjustly high recidivism scores compared to whites. Our contribution is quantifying the bias in the training data itself, and returning a closest feasible explanation for this bias. First, we took the proportion of Caucasians assigned to the three possible categories: low, medium, and high risk. Since the data is supposed to be fair, we use these proportions as the null hypothesis to test for bias in the Black population. That is, could the same process that generated the Caucasian proportions plausibly have generated the proportions of Blacks assigned to each category.
	<br><br>
	As expected, the null hypothesis was rejected at the \(\alpha = 0.05\) significance level, with a \(p\)-value of \(2.165 \times 10^{-15}\). Furthermore, we found that the closest plausible explanation for the data gave Blacks a higher chance of being categorized as "high risk" than Caucasians, which is illustrated in the graph below. The colors of the lollipop bars representing the closest plausible distribution correspond to how far away they are from the proposed distribution, with a heatmap key on the right.
	<br><br>
	We also tested the <a href="https://archive.ics.uci.edu/ml/datasets/Adult" target="_blank">UCI Adult</a> dataset which contains a variety of demographic information for over 40,000 people, with a binary label of whether they make more or less than $50,000 per year. Looking specifically at gender, we used the proportion of men labeled ">=50K" as our null hypothesis and tested the dataset of only women for deviation from this proportion. Again, the null hypothesis was rejected at the \(\alpha = 0.05\) significance level, with \(\kappa(x)=2.2022 \times 10^{-254}\). The closest feasible distribution is illustrated below, showing that any plausible explanation for the data would have to give females at least an 88% chance of being put in the "<=50k" category. Remember, we are not making statements about any algorithm, but rather a potential process that could have lead to the given labels in the dataset. In this case, that process would be a wide swath of real societal factors such as gender discrimination.
  </p>
  <div>
    <div style="margin-bottom: 0%;">
      <div style="display:inline-block;">
        <img src="./images/UCI_female_sex_distributions.jpg" alt="UCI Female Proposed vs. Plausible Distribution" class="UCI-results-diagram" width="600" height=auto>
      </div>
    </div>
  </div>
  
<h3>Additional Resources</h3>

  <div class="additional-resources-list">[ <a href="https://arxiv.org/pdf/1907.06010" target="_blank">Paper (PDF)</a> ]</div>
  <a href="https://arxiv.org/pdf/1907.06010" target="_blank"><img src="images/fobflas-pdf.jpg" class="pdf-thumbnail"></a>

  <!-- Authors and Headshots -->
  <h3>Authors</h3>
  <div>
    <div style="margin-bottom: 3%;">
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/jonathan-hayase-5ab849128/" target="_blank">
          <img src="images/jonathan.jpg" alt="Jonathan Hayase" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/jonathan-hayase-5ab849128/" class="text-link" target="_blank">
          <p style="text-align:center; margin: 0;">Jonathan Hayase</p>
        </a>
      </div>
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/julius-lauw-882652139/" target="_blank">
          <img src="images/julius.jpg" alt="Julius Lauw" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/julius-lauw-882652139/" class="text-link" target="_blank">
          <p style="text-align:center; margin: 0;">Julius Lauw</p>
        </a>
      </div>
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/dominique-macias/" target="_blank">
          <img src="images/dom.jpg" alt="Dominique Macias" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/dominique-macias/" class="text-link" target="_blank">
          <p style="text-align:center; margin: 0;">Dominique Macias</p>
        </a>
      </div>
    </div>
    <div>
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/akshay-trikha" target="_blank">
          <img src="images/akshay.jpg" alt="Akshay Trikha" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/akshay-trikha" class="text-link" target="_blank">
          <p style="text-align:center; margin: 0;">Akshay Trikha</p>
        </a>
      </div>
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/julia-vendemiatti-645a5613a/" target="_blank">
          <img src="images/julia.jpg" alt="Julia Vendemiatti" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/julia-vendemiatti-645a5613a/" class="text-link" target="_blank">
          <p style="text-align:center; margin: 0;">Julia Vendemiatti</p>
        </a>
      </div>
      <div style="display:inline-block;">
        <a href="https://www.linkedin.com/in/georgemontanez/" target="_blank">
          <img src="images/prof-george.png" alt="George Monta&ntilde;ez" class="headshot">
        </a>
        <a href="https://www.linkedin.com/in/georgemontanez/" class="text-link" target="_blank">
          <p style="text-align:center; margin: 0;">George Monta&ntilde;ez</p>
        </a>
      </div>
    </div>
  </div>

  <!-- Thank you(s) -->
  <h3>Acknowledgements</h3>
  <p>
    We'd like to thank our loving families and supportive friends, and the Walter Bradley Center for Natural and Artificial Intelligence for their generous support of this research.
  <p>
  <!-- Citations -->
  <h3>Further Reading</h3>
  <ul class="references">
      <li>Monta&ntilde;ez, G.D., Hayase, J., Lauw, J., Macias, D., Trikha, A., Vendemiatti, J. :The Futility of Bias-Free Learning and Search. 2nd Australasian Joint Conference on Artificial Intelligence (AI 2019), 2019. 277&mdash;288</li>
      <li>Monta&ntilde;ez, G.D. :The Famine of Forte: Few Search Problems Greatly Favor Your Algorithm.  In: 2017 IEEE International Conference on Systems, Man, and Cybernetics(SMC). pp. 477&mdash;482.</li>
      <li>Wolpert, D.H., Macready, W.G.: No free lunch theorems for optimization. Trans.Evol. Comp1(1), 67&mdash;82 (Apr 1997)</li>
      <li>Mitchell, T.D.: The need for biases in learning generalizations. In: Rutgers University: CBM-TR-117 (1980)</li>
  </ul>
  <div class="bottom-nav">
    <a href="projects.html">Back to Projects</a>
  </div>
</body>
</html>
