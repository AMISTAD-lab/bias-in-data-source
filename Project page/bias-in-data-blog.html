<!DOCTYPE html>
<html>
<head>
	<title>Identifying Bias in Data using Two-Distribution Hypothesis Tests - AMISTAD Lab</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<link rel="stylesheet" type="text/css" href="projects.css">
	<link rel="stylesheet" type="text/css" href="project-example-specific.css">
	<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
	<!-- Google Analytics setup -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-83215098-1', 'auto');
		ga('send', 'pageview');
	</script>
	<!-- MathJax Setup -->
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
<div div style="margin: auto;" class="main-page-box">
	<!-- AMISTAD x NSF x HMC -->
	<a href="../amistad.html"><img src="./images/amistad-logo.png" alt="AMISTAD Logo" class="header-logo" /></a>

	<h1>Identifying Bias in Data using Two-Distribution Hypothesis Tests</h1>
	<h2 class="subtitle">2021</h2>
	<h3>Overview</h3>
	<p>
		Imagine you’re flipping a coin you presume to be fair. You flip it 20 times and see the sequence
		$$H T T H T T H H H H H T H T H T H H T H.$$
		Nothing immediately stands out to you, so you never question whether the coin is fair or not. But imagine you flipped that same coin and saw the sequence
		$$H H H H H H H H H H H H H H H H H H H H.$$
		You’d immediately suspect something was up. More specifically, you’d almost assuredly say that the coin was unfairly weighted towards heads. But how did you come to this conclusion? Was it based on probability alone? If the coin was truly fair, both of the heads-tails sequences above have exactly a 1/(2^20) chance of happening, so what makes you suspicious of the second sequence?
		<br><br>
		Humans are pretty good at recognizing patterns, and that very pattern recognition probably gave you a hint that a weighted coin generated the second sequence. It wasn’t just the low probability of that specific sequence occurring that raised your eyebrows. It was also that sequence’s conformity to some pattern. Our team is interested in making use of these two factors, probability and conformity, to analyze machine learning training data, and identify the potentially biased processes that could have generated them.
	</p>

	<!-- Background -->
	<h3>Background</h3>
	<p>
		Statistical hypothesis tests allow people to probabilistically rule out a proposed hypothesis as an explanation for a given dataset. For example, a binomial test could be used to test the hypothesis that our coin in the previous example was fair. Such hypothesis tests work by generating a \(p\)-value, the probability that the observation (or a more extreme result) would occur under the proposed hypothesis. Typically, if this \(p\)-value is less than some chosen significance threshold alpha, then we reject the null hypothesis. Otherwise, we fail to reject it.
		<br><br>
		Several popular hypothesis tests, such as binomial, chi-square, and Student’s t, are widely used for testing the plausibility of null hypotheses. Some of these could be applicable for identifying bias in machine learning training data. Say a company wanted to train an algorithm to automate their hiring process. They would want to be able to feed this algorithm an applicant’s demographic information, background, and qualifications and have it return a result of “hire” or “do not hire”. In order to train this model, they decide to use their current and past hiring records, where each current or past applicant has an information profile, and a binary label of “hired” or “not hired”. Unfortunately, the company suspects that their historical hiring records might be unjustly biased towards white applicants. Here we take bias to be deviation from some proposed unbiased explanation. In this case, the unbiased explanation is that racial demographics of the hired employees of the company match that of the entire pool of applicants, so no favor is shown to one race over another in the hiring process. A statistical hypothesis test (specifically chi-square in this case) could be used to determine whether this unbiased explanation would plausibly explain the demographics of the hired applicants. By plausible, we mean that the hypothesis was not rejected by the test, not necessarily that the hypothesis has a “high” chance of producing the data.
		<br><br>
		While all the hypothesis tests mentioned above are useful, all of them assume that sequences are generated under a particular known distribution. This distribution is usually just the name of the test. For a chi square test, we assume a chi square distribution. Additionally, these tests can tell you whether or not the proposed hypothesis could plausibly explain the data, but unfortunately do not provide further insight. In many cases, if someone’s proposed hypothesis was rejected, they would probably be interested in answering the question of what hypothesis <i>would not</i> be rejected. For the company mentioned above, if their proposed unbiased explanation was rejected, they may wonder what explanation(s) would not be rejected, and how “close” their proposed hypothesis was to them.
		<br><br>
		That’s where our research comes in. Previous work from our lab has generated a novel hypothesis test that resolves the issues discussed above. We extend this work and show useful, real-world applications of the tests for industrial use.
	</p>
  
	<!-- Definitions -->
	<h3>Key Definitions and Methodology</h3>
	<p>
		Define the kardis \(\kappa(x)\) of event \(x\) as
		$$\kappa(x) := r\frac{p(x)}{\nu(x)}.$$
		where \(p(x)\) is the probability of observing \(x\) under probability distribution \(P\) (which represents the null hypothesis), \(\nu(x)\) is the <i>specificity</i> (conformity to a pattern) of \(x\) by some predetermined notion of structure, and \(r\) is a normalizing constant. Notice that this formulation of \(\kappa(x)\) captures both the probability and conformity of an event as discussed earlier. 
		<br><br>
		With this in mind, our lab has shown that for some chosen significance level \(\alpha\)
		$$\Pr(\kappa(x) \leq \alpha) \leq \alpha.$$
		Thus, we can reject a proposed hypothesis \(P\) if \(\kappa(x) \leq \alpha\). Furthermore, it has also been shown that in order for any new hypothesis to not be rejected by our test, it must boost \(p(x)\), the probability of observing \(x\), by <i>at least</i> a factor of
		$$s = \frac{\alpha\nu(x)}{rp(x)}.$$
		This bound is the key to our results, since it means that a proposed probablity distribution (explanation) \(Q\) must give at least
		$$q(x) \geq sp(x)$$
		probability to \(x\) in order to not be rejected by our hypothesis test.
		<br><br>
		Most importantly, this bound allows us to find the "closest feasible explanation" for the data in the case that the original proposed hypothesis is rejected by solving a constrained optimization problem. In other words, if a proposed hypothesis is rejected, we can say that the true explanation would have to be <i>biased</i> by at least a certain amount. This gives our test a unique degree of clarity, since it allows users to get a better sense of what hidden biases might be present in the processes that created their data.
		<br><br>
		It should be stated clearly that we are not making claims about bias of any particular machine learning algorithm, or even identifying algorithmic bias at all. Rather, we seek to uncover biases in the <i>processes</i> that generate training data, like hiring practices or weighted coins.
	</p>

	<!-- Results -->
	<h3>Selected Results</h3>
	<p>
		We tested our methods on three real-world datasets, two of which will be shown here. The first dataset we tested for bias in was the publicly available <a href="https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis" target="_blank">COMPAS</a> (Correctional Offender Management Profiling for Alternative Sanctions) dataset which was used to train the COMPAS algorithm. The algorithm's purpose was to assign defendants in court a risk score for recidivism (tendency to reoffend), and it remains a popular tool for judges and parole officers. 
		<br><br>
		It is already <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" target="_blank">well known</a> that the algorithm is biased against Black defendants, consistently giving them unjustly high recidivism scores compared to Caucasians. Our contribution is quantifying the bias in the training data itself, and returning a closest feasible explanation for this bias. First, we took the proportion of Caucasians assigned to the three possible categories: low, medium, and high risk. Since the data is supposed to be fair, we use these proportions as the null hypothesis to test for bias in the Black population. That is, could the same process that generated the Caucasian proportions plausibly have generated the proportions of Black people assigned to each category?
		<br><br>
		As expected, the null hypothesis was rejected at the \(\alpha = 0.05\) significance level, with a \(\kappa(x)=2.4272 \times 10^{-683}\). Furthermore, we found that the closest plausible explanation for the data gave Black people a higher chance of being categorized as "high risk" than Caucasians, which is illustrated in the graph below. The colors of the lollipop bars representing the closest plausible distribution correspond to how far away they are from the proposed distribution, with a heatmap key on the right.
	</p>
	<div>
		<div style="margin-bottom: 0%;">
		<div style="display:inline-block;">
			<img src="./images/compass_risk_african_american_scoretext_distributions.jpg" alt="COMPAS African American Proposed vs. Plausible Distribution" class="COMPAS-results-diagram" width="600" height=auto>
		</div>
		</div>
	</div>
	<p>
		We also tested the <a href="https://archive.ics.uci.edu/ml/datasets/Adult" target="_blank">UCI Adult</a> dataset which contains a variety of demographic information for over 40,000 people, with a binary label of whether they make more or less than $50,000 per year. Looking specifically at gender, we used the proportion of men labeled ">=50K" as our null hypothesis and tested the dataset of only women for deviation from this proportion. Again, the null hypothesis was rejected at the \(\alpha = 0.05\) significance level, with \(\kappa(x)=2.2022 \times 10^{-254}\). The closest feasible distribution is illustrated below, showing that any plausible explanation for the data would have to give females at least an 88% chance of being put in the "<=50k" category. Remember, we are not making statements about any algorithm, but rather a potential process that could have lead to the given labels in the dataset. In this case, that process would be a wide swath of real societal factors such as gender discrimination.
	</p>
	<div>
		<div style="margin-bottom: 0%;">
		<div style="display:inline-block;">
			<img src="./images/UCI_female_sex_distributions.jpg" alt="UCI Female Proposed vs. Plausible Distribution" class="UCI-results-diagram" width="600" height=auto>
		</div>
		</div>
	</div>
	<p>
		Our hypothesis tests also have some significant benefits compared to existing methods, which we hope will make them fit for industrial use.
		<ul style="margin-top:-30px;"; align="left">
			<li>Unlike other tests in the literature, our tests can handle compound hypotheses. That is, they can reject whole sets of hypotheses, rather than just one at a time.</li>
			<br>
			<li>They are fast and tractable. Even for datasets consisting of 100,000 entries and 10 labels, our tests can run in about 20 minutes on a consumer level laptop. This contrasts other tests in the literature which appear to need immense processing power.</li>
			<br>
			<li>With our tests, there is no need to know the behavior of the underlying probability distribution \(P\) in advance, only the probability \(p(x)\) for the singular event \(x\). This distinguishes our tests from many common hypothesis tests (chi square, binomial, etc.) which assume a distribution's shape.
		</ul>
	</p>
	
	<!-- Authors and Headshots -->
	<h3>Authors</h3>
	<div>
		<div style="margin-bottom: 3%;">
		<div style="display:inline-block;">
			<a href="https://www.linkedin.com/in/williamyik/" target="_blank">
			<img src="images/william.jpg" alt="William Yik" class="headshot">
			</a>
			<a href="https://www.linkedin.com/in/williamyik/" class="text-link" target="_blank">
			<p style="text-align:center; margin: 0;">William Yik</p>
			</a>
		</div>
		<div style="display:inline-block;">
			<a href="https://www.linkedin.com/in/tim-lindsey-412442214/" target="_blank">
			<img src="images/tim.jpeg" alt="Tim Lindsey" class="headshot">
			</a>
			<a href="https://www.linkedin.com/in/tim-lindsey-412442214/" class="text-link" target="_blank">
			<p style="text-align:center; margin: 0;">Tim Lindsey</p>
			</a>
		</div>
		<div style="display:inline-block;">
			<a href="https://www.cs.hmc.edu/~montanez/amistad.html" target="_blank">
			<img src="images/lim.jpg" alt="Limnanthes Serafini" class="headshot">
			</a>
			<a href="https://www.cs.hmc.edu/~montanez/amistad.html" class="text-link" target="_blank">
			<p style="text-align:center; margin: 0;">Limnanthes Serafini</p>
			</a>
		</div>
		</div>
		<div>
		<div style="display:inline-block;">
			<a href="https://www.linkedin.com/in/georgemontanez/" target="_blank">
			<img src="images/prof-george.png" alt="George Monta&ntilde;ez" class="headshot">
			</a>
			<a href="https://www.linkedin.com/in/georgemontanez/" class="text-link" target="_blank">
			<p style="text-align:center; margin: 0;">George Monta&ntilde;ez</p>
			</a>
		</div>
		</div>
	</div>

	<!-- Citations -->
	<h3>Further Reading</h3>
	<ul class="references">
		<li>Monta&ntilde;ez, G. D. (2018). A unified model of complex specified information. BIO-Complexity, 2018.</li>
		<li>Hom, C., Maina-Kilaas, A. R., Ginta, K., Lay, C., & Monta&ntilde;ez, G. D. (2021, February). The Gopher's Gambit: Survival Advantages of Artifact-based Intention Perception. In ICAART (1) (pp. 205&mdash;215).</li>
		<li>Jiang, H., & Nachum, O. (2020, June). Identifying and correcting label bias in machine learning. In International Conference on Artificial Intelligence and Statistics (pp. 702&mdash;712). PMLR.</li>
		<li>Taskesen, B., Blanchet, J., Kuhn, D., & Nguyen, V. A. (2021, March). A statistical test for probabilistic fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (pp. 648&mdash;665).</li>
	</ul>
	<div class="bottom-nav">
		<a href="projects.html">Back to Projects</a>
	</div>
</body>
</html>
